try:
  import tensorflow as tf
except:

  print 'No TensorFlow'
  pass
import random
import numpy as np

def data_set(data_url):
  """process data input."""
  data = []
  word_count = []
  alignment = {}
  fin = open(data_url)
  while True:
    line = fin.readline()
    if not line:
      break
    id_freqs = line.split()
    doc = {}
    count = 0
    for id_freq in id_freqs[1:]:
      items = id_freq.split(':')
      # python starts from 0
      doc[int(items[0])-1] = int(items[1])
      count += int(items[1])
    if count > 0:
      alignment[int(id_freqs[0])] = len(data)
      data.append(doc)
      word_count.append(count)
  fin.close()
  return data, word_count, alignment

def create_batches(data_size, batch_size, shuffle=True):
  """create index by batches."""
  batches = []
  ids = range(data_size)
  if shuffle:
    random.shuffle(ids)
  for i in xrange(data_size / batch_size):
    start = i * batch_size
    end = (i + 1) * batch_size
    batches.append(ids[start:end])
  # the batch of which the length is less than batch_size
  rest = data_size % batch_size
  if rest > 0:
    batches.append(ids[-rest:] + [-1] * (batch_size - rest))  # -1 as padding
  return batches

def fetch_data(data, count, idx_batch, vocab_size):
  """fetch input data by batch."""
  batch_size = len(idx_batch)
  data_batch = np.zeros((batch_size, vocab_size))
  count_batch = []
  mask = np.zeros(batch_size)
  indices = []
  values = []
  for i, doc_id in enumerate(idx_batch):
    if doc_id != -1:
      for word_id, freq in data[doc_id].items():
        data_batch[i, word_id] = freq
      count_batch.append(count[doc_id])
      mask[i]=1.0
    else:
      count_batch.append(0)
  return data_batch, count_batch, mask

def variable_parser(var_list, prefix):
  """return a subset of the all_variables by prefix."""
  ret_list = []
  for var in var_list:
    varname = var.name
    varprefix = varname.split('/')[0]
    if varprefix == prefix:
      ret_list.append(var)
  return ret_list

def linear(inputs,
           output_size,
           no_bias=False,
           bias_start_zero=False,
           matrix_start_zero=False,
           scope=None,
           input_is_sparse=False,
           trainable=True):
  """Define a linear connection."""
  with tf.variable_scope(scope or 'Linear'):
    if matrix_start_zero:
      matrix_initializer = tf.constant_initializer(0)
    else:
      matrix_initializer = None
    if bias_start_zero:
      bias_initializer = tf.constant_initializer(0)
    else:
      bias_initializer = None
    input_size = inputs.get_shape()[1].value
    matrix = tf.get_variable('Matrix', [input_size, output_size],
                             initializer=matrix_initializer, trainable=trainable)
    bias_term = tf.get_variable('Bias', [output_size],
                                initializer=bias_initializer, trainable=trainable)
    if input_is_sparse:
      output = tf.sparse_tensor_dense_matmul(inputs, matrix)
    else:
      output = tf.matmul(inputs, matrix)
    if not no_bias:
      output = output + bias_term
  return output

def mlp(inputs,
        mlp_hidden=(),
        mlp_nonlinearity=None,
        scope=None,
        input_is_sparse=False,
        trainable=True):
  """Define an MLP."""
  if mlp_nonlinearity is None:
    mlp_nonlinearity=tf.nn.tanh
  with tf.variable_scope(scope or 'Linear'):
    mlp_layer = len(mlp_hidden)
    res = inputs
    for l in xrange(mlp_layer):
      res = mlp_nonlinearity(linear(res, mlp_hidden[l], scope='l'+str(l), input_is_sparse=(input_is_sparse and l==0), trainable=trainable))
    return res
